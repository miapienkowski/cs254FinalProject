{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS254 Final Project: May 1st\n",
    " gathering previous data preparation into singlke function\n",
    " trying to implement VGG as a feature extractor\n",
    " then use SVM as a classifier\n",
    " \n",
    " potentially need to reformat data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    #initial imports\n",
    "    import os\n",
    "    import random\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    import keras\n",
    "    from keras.models import Sequential\n",
    "    from keras.utils import to_categorical\n",
    "    from keras.preprocessing import image\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from tqdm import tqdm\n",
    "    import keras\n",
    "    from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "    from keras.layers.normalization import BatchNormalization\n",
    "    import numpy as np\n",
    "    from keras.applications.vgg16 import VGG16\n",
    "    from keras.applications.vgg16 import preprocess_input\n",
    "    from keras.preprocessing.image import load_img\n",
    "    from keras.preprocessing.image import img_to_array\n",
    "    from keras.models import Model\n",
    "    from pickle import dump\n",
    "    import pickle\n",
    "    %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "######################################################################################################\n",
    "#####################################################################################################\n",
    "#getting data ready\n",
    "#create a dataframe containing the names of all of the jpgs in the full dataset\n",
    "fileList = []\n",
    "for file in os.listdir('C:/Users/miapi/Downloads/ttpla_dataset/sized_data'):\n",
    "    if file.endswith(\".jpg\"):\n",
    "        fileList.append(file)\n",
    "\n",
    "df = pd.DataFrame(fileList)\n",
    "\n",
    "#also creating an index to use later in extracting the img labels from the json files\n",
    "index = df.index\n",
    "\n",
    "#formatting the dataframe\n",
    "df = df.rename(columns={0: \"Id\"}) \n",
    "df['labels'] = df.Id\n",
    "df['labels'] = df['labels'].str.replace('.jpg','')\n",
    "df['tower_wooden'] = \"\"\n",
    "df['tower_lattice'] = \"\"\n",
    "df['tower_tucohy'] = \"\"\n",
    "index = df.index\n",
    " ######################################################################################################\n",
    "#####################################################################################################\n",
    "for file in os.listdir('C:/Users/miapi/Downloads/ttpla_dataset/sized_data'):\n",
    "    if file.endswith(\".json\"):\n",
    "\n",
    "        #load file as json object\n",
    "        filePath = (\"C:/Users/miapi/Downloads/ttpla_dataset/sized_data/\"+file)\n",
    "        f = open(filePath,)\n",
    "        json_file = json.load(f)\n",
    "        #get file namme without extension\n",
    "        file2 = file.replace('.json','')\n",
    "\n",
    "        #find designated jpeg (row in df)\n",
    "        #rslt_df = df.loc[df['labels'] == file2]\n",
    "        apples_indices = index[df['labels'] == file2]\n",
    "\n",
    "        #parse json file to see if tower_wooden, tower_lattice, or tower_tucohy exists\n",
    "        #add 0 or 1 to the corresponding dataframe row accordingly\n",
    "\n",
    "        for element in json_file['shapes']:\n",
    "\n",
    "            #tower wooden\n",
    "            bool_wood = any(sd['label']=='tower_wooden' for sd in json_file['shapes'])  \n",
    "            if bool_wood:\n",
    "                df.at[apples_indices,'tower_wooden']= 1\n",
    "            else:\n",
    "                df.at[apples_indices,'tower_wooden']= 0 \n",
    "\n",
    "            #tower lattice\n",
    "            bool_lat = any(sd['label']=='tower_lattice' for sd in json_file['shapes']) \n",
    "            if bool_lat:\n",
    "                df.at[apples_indices,'tower_lattice']= 1\n",
    "            else:\n",
    "                df.at[apples_indices,'tower_lattice']= 0\n",
    "\n",
    "            #tower tucohy\n",
    "            bool_tuc = any(sd['label']=='tower_tucohy' for sd in json_file['shapes']) \n",
    "            if bool_tuc:\n",
    "                df.at[apples_indices,'tower_tucohy']= 1\n",
    "            else:\n",
    "                df.at[apples_indices,'tower_tucohy']= 0\n",
    "######################################################################################################\n",
    "#####################################################################################################                  \n",
    "\n",
    "#create column containing info for all of the towers\n",
    "df['towers'] = df[['tower_wooden', 'tower_lattice','tower_tucohy']].values.tolist()\n",
    "df['tower_labels'] = np.empty((len(df), 0)).tolist()\n",
    "for i in range(len(df)):\n",
    "\n",
    "    if df['towers'][i][0] == 1:\n",
    "        df['tower_labels'][i].append(\"tower_wooden\")\n",
    "\n",
    "    if df['towers'][i][1] == 1:\n",
    "        df['tower_labels'][i].append(\"tower_lattice\")\n",
    "\n",
    "    if df['towers'][i][2] == 1:\n",
    "        df['tower_labels'][i].append(\"tower_tucohy\")\n",
    "\n",
    "  ######################################################################################################\n",
    "#####################################################################################################      \n",
    "df['tower_wooden'] = df['tower_wooden'].astype(int)\n",
    "df['tower_lattice'] = df['tower_lattice'].astype(int)\n",
    "df['tower_tucohy'] = df['tower_tucohy'].astype(int)\n",
    "#need to split the data into testing, training, and validating\n",
    "random.seed(123)\n",
    "train, validate, test = \\\n",
    "              np.split(df.sample(frac=1, random_state=42), \n",
    "                       [int(.6*len(df)), int(.8*len(df))])    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the y data\n",
    "y = np.array(train.drop(['Id', 'labels','towers','tower_labels'],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = VGG16()\n",
    "# remove the output layer\n",
    "model = Model(inputs=model.inputs, outputs=model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Currently, have only done the feature extraction for training images.  \n",
    "this is in the below cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image = []\n",
    "#this is the one that worked!!!\n",
    "for i in range(train.shape[0]):\n",
    "    # load an image from file\n",
    "    image = load_img('C:/Users/miapi/Downloads/ttpla_dataset/sized_data/'+train.iloc[i][0], target_size=(224, 224))\n",
    "    # convert the image pixels to a numpy array\n",
    "    image = img_to_array(image)\n",
    "    # reshape data for the model\n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    # prepare the image for the VGG model\n",
    "    image = preprocess_input(image)\n",
    "    # get extracted features\n",
    "    features = model.predict(image)\n",
    "    # save to file\n",
    "    train_image.append(features)\n",
    "feats = np.array(train_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this writes the data into a file and also reads the file\n",
    "I also have code that writes each feature vector into the file individually - I think for formatting might need to write each feature vector into its own file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cats is made by adding numpy array of vectors to pickle file\n",
    "filename = 'cats'\n",
    "outfile = open(filename,'wb')\n",
    "dump(feats, outfile)\n",
    "outfile.close()\n",
    "\n",
    "\n",
    "infile = open(filename,'rb')\n",
    "new_cat = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
