{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying without tower tucohy\n",
    "## could do binary - tower wooden or no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    #initial imports\n",
    "    import os\n",
    "    import random\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    import keras\n",
    "    from keras.models import Sequential\n",
    "    from keras.utils import to_categorical\n",
    "    from keras.preprocessing import image\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from tqdm import tqdm\n",
    "    import keras\n",
    "    from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "    from keras.layers.normalization import BatchNormalization\n",
    "    import numpy as np\n",
    "    %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare():\n",
    "    ######################################################################################################\n",
    "    #####################################################################################################\n",
    "    #getting data ready\n",
    "    #create a dataframe containing the names of all of the jpgs in the full dataset\n",
    "    fileList = []\n",
    "    for file in os.listdir('C:/Users/miapi/Downloads/ttpla_dataset/sized_data - Copy'):\n",
    "        if file.endswith(\".jpg\"):\n",
    "            fileList.append(file)\n",
    "\n",
    "    df = pd.DataFrame(fileList)\n",
    "\n",
    "    #also creating an index to use later in extracting the img labels from the json files\n",
    "    index = df.index\n",
    "\n",
    "    #formatting the dataframe\n",
    "    df = df.rename(columns={0: \"Id\"}) \n",
    "    df['labels'] = df.Id\n",
    "    df['labels'] = df['labels'].str.replace('.jpg','')\n",
    "    df['tower_wooden'] = \"\"\n",
    "    df['tower_lattice'] = \"\"\n",
    "    df['tower_tucohy'] = \"\"\n",
    "    index = df.index\n",
    "     ######################################################################################################\n",
    "    #####################################################################################################\n",
    "    for file in os.listdir('C:/Users/miapi/Downloads/ttpla_dataset/sized_data - Copy'):\n",
    "        if file.endswith(\".json\"):\n",
    "\n",
    "            #load file as json object\n",
    "            filePath = (\"C:/Users/miapi/Downloads/ttpla_dataset/sized_data - Copy/\"+file)\n",
    "            f = open(filePath,)\n",
    "            json_file = json.load(f)\n",
    "            #get file namme without extension\n",
    "            file2 = file.replace('.json','')\n",
    "\n",
    "            #find designated jpeg (row in df)\n",
    "            #rslt_df = df.loc[df['labels'] == file2]\n",
    "            apples_indices = index[df['labels'] == file2]\n",
    "\n",
    "            #parse json file to see if tower_wooden, tower_lattice, or tower_tucohy exists\n",
    "            #add 0 or 1 to the corresponding dataframe row accordingly\n",
    "\n",
    "            for element in json_file['shapes']:\n",
    "\n",
    "                #tower wooden\n",
    "                bool_wood = any(sd['label']=='tower_wooden' for sd in json_file['shapes'])  \n",
    "                if bool_wood:\n",
    "                    df.at[apples_indices,'tower_wooden']= 1\n",
    "                else:\n",
    "                    df.at[apples_indices,'tower_wooden']= 0 \n",
    "\n",
    "                #tower lattice\n",
    "                bool_lat = any(sd['label']=='tower_lattice' for sd in json_file['shapes']) \n",
    "                if bool_lat:\n",
    "                    df.at[apples_indices,'tower_lattice']= 1\n",
    "                else:\n",
    "                    df.at[apples_indices,'tower_lattice']= 0\n",
    "\n",
    "                #tower tucohy\n",
    "                bool_tuc = any(sd['label']=='tower_tucohy' for sd in json_file['shapes']) \n",
    "                if bool_tuc:\n",
    "                    df.at[apples_indices,'tower_tucohy']= 0\n",
    "                else:\n",
    "                    df.at[apples_indices,'tower_tucohy']= 0\n",
    "   ######################################################################################################\n",
    "    #####################################################################################################                  df.at[apples_indices,'tower_tucohy']= 0\n",
    "\n",
    "    #create column containing info for all of the towers\n",
    "    df['towers'] = df[['tower_wooden', 'tower_lattice','tower_tucohy']].values.tolist()\n",
    "    df['tower_labels'] = np.empty((len(df), 0)).tolist()\n",
    "    for i in range(len(df)):\n",
    "\n",
    "        if df['towers'][i][0] == 1:\n",
    "            df['tower_labels'][i].append(\"tower_wooden\")\n",
    "\n",
    "        if df['towers'][i][1] == 1:\n",
    "            df['tower_labels'][i].append(\"tower_lattice\")\n",
    "\n",
    "        if df['towers'][i][2] == 1:\n",
    "            df['tower_labels'][i].append(\"tower_tucohy\")\n",
    "      ######################################################################################################\n",
    "    #####################################################################################################        \n",
    "   #need to split the data into testing, training, and validating\n",
    "    random.seed(123)\n",
    "    train, validate, test = \\\n",
    "                  np.split(df.sample(frac=1, random_state=42), \n",
    "                           [int(.6*len(df)), int(.8*len(df))])    \n",
    "    #reading in all of the training images\n",
    "    train_image = []\n",
    "    for i in tqdm(range(train.shape[0])):\n",
    "        img = image.load_img('C:/Users/miapi/Downloads/ttpla_dataset/sized_data - Copy/'+train.iloc[i][0],target_size=(400,400,3))\n",
    "        img = image.img_to_array(img)\n",
    "        img = img/255\n",
    "        train_image.append(img)\n",
    "    X = np.array(train_image)\n",
    "    #reading in all of the validation images\n",
    "    valid_image = []\n",
    "    for i in tqdm(range(validate.shape[0])):\n",
    "        img = image.load_img('C:/Users/miapi/Downloads/ttpla_dataset/sized_data - Copy/'+validate.iloc[i][0],target_size=(400,400,3))\n",
    "        img = image.img_to_array(img)\n",
    "        img = img/255\n",
    "        valid_image.append(img)\n",
    "    X_valid = np.array(valid_image)\n",
    "    #reading in all of the test images\n",
    "    test_image = []\n",
    "    for i in tqdm(range(test.shape[0])):\n",
    "        img = image.load_img('C:/Users/miapi/Downloads/ttpla_dataset/sized_data - Copy/'+test.iloc[i][0],target_size=(400,400,3))\n",
    "        img = image.img_to_array(img)\n",
    "        img = img/255\n",
    "        test_image.append(img)\n",
    "    X_test = np.array(test_image)\n",
    "    #need to df['DataFrame Column'] = df['DataFrame Column'].astype(float)first convert the column values from strings to floats\n",
    "    df['tower_wooden'] = df['tower_wooden'].astype(int)\n",
    "    df['tower_lattice'] = df['tower_lattice'].astype(int)\n",
    "    df['tower_tucohy'] = df['tower_tucohy'].astype(int)\n",
    "    y = np.array(train.drop(['Id', 'labels','towers','tower_labels'],axis=1))\n",
    "    \n",
    "    #validation set y\n",
    "    y_validate = np.array(validate.drop(['Id', 'labels','towers','tower_labels'],axis=1))\n",
    "    #test set y\n",
    "    y_test = np.array(test.drop(['Id', 'labels','towers','tower_labels'],axis=1))\n",
    "    \n",
    "    \n",
    "    X_train=np.asarray(X).astype(np.int)\n",
    "    y_train=np.asarray(y).astype(np.int)\n",
    "    X_valid=np.asarray(X_valid).astype(np.int)\n",
    "    y_valid=np.asarray(y_validate).astype(np.int)\n",
    "    X_test=np.asarray(X_test).astype(np.int)\n",
    "    y_test=np.asarray(y_test).astype(np.int)\n",
    "    \n",
    "    #special =  image.load_img('C:/Users/miapi/Downloads/ttpla_dataset/sized_data/'+train.iloc[0][0], target_size=(224, 224))\n",
    "    return X_train, y_train, X_valid, y_valid, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 744/744 [00:03<00:00, 193.56it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 248/248 [00:01<00:00, 168.07it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 249/249 [00:01<00:00, 153.94it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_valid, y_valid, X_test, y_test = prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(400,400,3)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(3, activation='sigmoid'))   #changed to 2 bc no tower tucohy\n",
    "# compile model\n",
    "opt = keras.optimizers.SGD(lr=0.01, momentum=0.9)\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(rescale=1.0/255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_it = datagen.flow(X_train, y_train, batch_size=32)\n",
    "valid_it = datagen.flow(X_valid, y_valid, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "history = model.fit(train_it, steps_per_epoch=len(train_it),\n",
    "    validation_data=valid_it, validation_steps=len(valid_it), epochs=50, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trying full transfer learning with vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# vgg16 transfer learning on the planet dataset\n",
    "import sys\n",
    "from numpy import load\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import backend\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate fbeta score for multi-class/label classification\n",
    "def fbeta(y_true, y_pred, beta=2):\n",
    "\t# clip predictions\n",
    "\ty_pred = backend.clip(y_pred, 0, 1)\n",
    "\t# calculate elements\n",
    "\ttp = backend.sum(backend.round(backend.clip(y_true * y_pred, 0, 1)), axis=1)\n",
    "\tfp = backend.sum(backend.round(backend.clip(y_pred - y_true, 0, 1)), axis=1)\n",
    "\tfn = backend.sum(backend.round(backend.clip(y_true - y_pred, 0, 1)), axis=1)\n",
    "\t# calculate precision\n",
    "\tp = tp / (tp + fp + backend.epsilon())\n",
    "\t# calculate recall\n",
    "\tr = tp / (tp + fn + backend.epsilon())\n",
    "\t# calculate fbeta, averaged across each class\n",
    "\tbb = beta ** 2\n",
    "\tfbeta_score = backend.mean((1 + bb) * (p * r) / (bb * p + r + backend.epsilon()))\n",
    "\treturn fbeta_score\n",
    " \n",
    "# define cnn model\n",
    "def define_model(in_shape=(400, 400, 3), out_shape=3):\n",
    "\t# load model\n",
    "\tmodel = VGG16(include_top=False, input_shape=in_shape)\n",
    "\t# mark loaded layers as not trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new classifier layers\n",
    "\tflat1 = Flatten()(model.layers[-1].output)\n",
    "\tclass1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "\toutput = Dense(out_shape, activation='sigmoid')(class1)\n",
    "\t# define new model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.01, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='binary_crossentropy', metrics=[fbeta])\n",
    "\treturn model\n",
    " \n",
    "# plot diagnostic learning curves\n",
    "def summarize_diagnostics(history):\n",
    "\t# plot loss\n",
    "\tpyplot.subplot(211)\n",
    "\tpyplot.title('Cross Entropy Loss')\n",
    "\tpyplot.plot(history.history['loss'], color='blue', label='train')\n",
    "\tpyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
    "\t# plot accuracy\n",
    "\tpyplot.subplot(212)\n",
    "\tpyplot.title('Fbeta')\n",
    "\tpyplot.plot(history.history['fbeta'], color='blue', label='train')\n",
    "\tpyplot.plot(history.history['val_fbeta'], color='orange', label='test')\n",
    "\t# save plot to file\n",
    "\tfilename = sys.argv[0].split('/')[-1]\n",
    "\tpyplot.savefig(filename + '_plot.png')\n",
    "\tpyplot.close()\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the test harness for evaluating a model\n",
    "def run_test_harness(X_train, y_train, X_valid, y_valid, X_test, y_test):\n",
    "\t# load dataset\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
    "\t# specify imagenet mean values for centering\n",
    "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
    "\t# prepare iterators\n",
    "\ttrain_it = datagen.flow(X_train, y_train, batch_size=16)\n",
    "\tvalid_it = datagen.flow(X_valid, y_valid, batch_size = 16)\n",
    "\ttest_it = datagen.flow(X_test, y_test, batch_size=16)\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# fit model\n",
    "\thistory = model.fit(train_it, steps_per_epoch=len(train_it),\n",
    "\t\tvalidation_data=valid_it, validation_steps=len(valid_it), epochs=20, verbose=1)\n",
    "\t# evaluate model\n",
    "\tloss, fbeta = model.evaluate(test_it, steps=len(test_it), verbose=1)\n",
    "\tprint('> loss=%.3f, fbeta=%.3f' % (loss, fbeta))\n",
    "\t# learning curves\n",
    "\tsummarize_diagnostics(history)\n",
    " \n",
    "# entry point, run the test harness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "47/47 [==============================] - 929s 20s/step - loss: 5.7593 - fbeta: 0.0301 - val_loss: 0.5879 - val_fbeta: 0.0000e+00\n",
      "Epoch 2/20\n",
      "47/47 [==============================] - 919s 20s/step - loss: 0.5486 - fbeta: 0.0000e+00 - val_loss: 0.5000 - val_fbeta: 0.0000e+00\n",
      "Epoch 3/20\n",
      "47/47 [==============================] - 918s 20s/step - loss: 0.4645 - fbeta: 0.0000e+00 - val_loss: 0.4596 - val_fbeta: 0.0000e+00\n",
      "Epoch 4/20\n",
      "47/47 [==============================] - 925s 20s/step - loss: 0.4302 - fbeta: 0.0000e+00 - val_loss: 0.4380 - val_fbeta: 0.0000e+00\n",
      "Epoch 5/20\n",
      "47/47 [==============================] - 913s 20s/step - loss: 0.3985 - fbeta: 0.0000e+00 - val_loss: 0.4257 - val_fbeta: 0.0000e+00\n",
      "Epoch 6/20\n",
      "47/47 [==============================] - 917s 20s/step - loss: 0.3788 - fbeta: 0.0000e+00 - val_loss: 0.4183 - val_fbeta: 0.0000e+00\n",
      "Epoch 7/20\n",
      "47/47 [==============================] - 917s 20s/step - loss: 0.3656 - fbeta: 0.0000e+00 - val_loss: 0.4133 - val_fbeta: 0.0000e+00\n",
      "Epoch 8/20\n",
      "47/47 [==============================] - 921s 20s/step - loss: 0.3739 - fbeta: 0.0000e+00 - val_loss: 0.4097 - val_fbeta: 0.0000e+00\n",
      "Epoch 9/20\n",
      "47/47 [==============================] - 918s 20s/step - loss: 0.3769 - fbeta: 0.0000e+00 - val_loss: 0.4069 - val_fbeta: 0.0000e+00\n",
      "Epoch 10/20\n",
      "47/47 [==============================] - 919s 20s/step - loss: 0.3724 - fbeta: 0.0000e+00 - val_loss: 0.4049 - val_fbeta: 0.0000e+00\n",
      "Epoch 11/20\n",
      "47/47 [==============================] - 916s 20s/step - loss: 0.3602 - fbeta: 0.0000e+00 - val_loss: 0.4031 - val_fbeta: 0.0000e+00\n",
      "Epoch 12/20\n",
      "47/47 [==============================] - 923s 20s/step - loss: 0.3625 - fbeta: 0.0000e+00 - val_loss: 0.4018 - val_fbeta: 0.0000e+00\n",
      "Epoch 13/20\n",
      "47/47 [==============================] - 913s 20s/step - loss: 0.3666 - fbeta: 0.0000e+00 - val_loss: 0.4005 - val_fbeta: 0.0000e+00\n",
      "Epoch 14/20\n",
      "47/47 [==============================] - 914s 20s/step - loss: 0.3594 - fbeta: 0.0000e+00 - val_loss: 0.3996 - val_fbeta: 0.0000e+00\n",
      "Epoch 15/20\n",
      "47/47 [==============================] - 918s 20s/step - loss: 0.3684 - fbeta: 0.0000e+00 - val_loss: 0.3984 - val_fbeta: 0.0000e+00\n",
      "Epoch 16/20\n",
      "47/47 [==============================] - 923s 20s/step - loss: 0.3563 - fbeta: 0.0000e+00 - val_loss: 0.3976 - val_fbeta: 0.0000e+00\n",
      "Epoch 17/20\n",
      "47/47 [==============================] - 922s 20s/step - loss: 0.3411 - fbeta: 0.0000e+00 - val_loss: 0.3970 - val_fbeta: 0.0000e+00\n",
      "Epoch 18/20\n",
      "47/47 [==============================] - 920s 20s/step - loss: 0.3465 - fbeta: 0.0000e+00 - val_loss: 0.3963 - val_fbeta: 0.0000e+00\n",
      "Epoch 19/20\n",
      "47/47 [==============================] - 914s 20s/step - loss: 0.3641 - fbeta: 0.0000e+00 - val_loss: 0.3954 - val_fbeta: 0.0000e+00\n",
      "Epoch 20/20\n",
      "47/47 [==============================] - 926s 20s/step - loss: 0.3445 - fbeta: 0.0000e+00 - val_loss: 0.3952 - val_fbeta: 0.0000e+00\n",
      "16/16 [==============================] - 229s 14s/step - loss: 0.3728 - fbeta: 0.0000e+00\n",
      "> loss=0.373, fbeta=0.000\n"
     ]
    }
   ],
   "source": [
    "run_test_harness(X_train, y_train, X_valid, y_valid, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
