{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changes:\n",
    "- change tower_labels to space-separated list of tags\n",
    "- final models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data - Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "#initial imports\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import keras\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "%matplotlib inline\n",
    "    \n",
    "    \n",
    "# vgg16 imports\n",
    "import sys\n",
    "from numpy import load\n",
    "from keras import backend\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from os import listdir\n",
    "from numpy import zeros\n",
    "from numpy import asarray\n",
    "from numpy import savez_compressed\n",
    "from pandas import read_csv\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "from numpy import ones\n",
    "from sklearn.metrics import fbeta_score\n",
    "from keras import backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "######################################################################################################\n",
    "#####################################################################################################\n",
    "#getting data ready\n",
    "#create a dataframe containing the names of all of the jpgs in the full dataset\n",
    "fileList = []\n",
    "for file in os.listdir('C:/Users/miapi/Downloads/ttpla_dataset/sized_data - Copy'):\n",
    "    if file.endswith(\".jpg\"):\n",
    "        fileList.append(file)\n",
    "\n",
    "df = pd.DataFrame(fileList)\n",
    "\n",
    "#also creating an index to use later in extracting the img labels from the json files\n",
    "index = df.index\n",
    "\n",
    "#formatting the dataframe\n",
    "df = df.rename(columns={0: \"Id\"}) \n",
    "df['labels'] = df.Id\n",
    "df['labels'] = df['labels'].str.replace('.jpg','')\n",
    "df['tower_wooden'] = \"\"\n",
    "df['tower_lattice'] = \"\"\n",
    "df['tower_tucohy'] = \"\"\n",
    "index = df.index\n",
    " ######################################################################################################\n",
    "#####################################################################################################\n",
    "for file in os.listdir('C:/Users/miapi/Downloads/ttpla_dataset/sized_data - Copy'):\n",
    "    if file.endswith(\".json\"):\n",
    "\n",
    "        #load file as json object\n",
    "        filePath = (\"C:/Users/miapi/Downloads/ttpla_dataset/sized_data - Copy/\"+file)\n",
    "        f = open(filePath,)\n",
    "        json_file = json.load(f)\n",
    "        #get file namme without extension\n",
    "        file2 = file.replace('.json','')\n",
    "\n",
    "        #find designated jpeg (row in df)\n",
    "        #rslt_df = df.loc[df['labels'] == file2]\n",
    "        apples_indices = index[df['labels'] == file2]\n",
    "\n",
    "        #parse json file to see if tower_wooden, tower_lattice, or tower_tucohy exists\n",
    "        #add 0 or 1 to the corresponding dataframe row accordingly\n",
    "\n",
    "        for element in json_file['shapes']:\n",
    "\n",
    "            #tower wooden\n",
    "            bool_wood = any(sd['label']=='tower_wooden' for sd in json_file['shapes'])  \n",
    "            if bool_wood:\n",
    "                df.at[apples_indices,'tower_wooden']= 1\n",
    "            else:\n",
    "                df.at[apples_indices,'tower_wooden']= 0 \n",
    "\n",
    "            #tower lattice\n",
    "            bool_lat = any(sd['label']=='tower_lattice' for sd in json_file['shapes']) \n",
    "            if bool_lat:\n",
    "                df.at[apples_indices,'tower_lattice']= 1\n",
    "            else:\n",
    "                df.at[apples_indices,'tower_lattice']= 0\n",
    "\n",
    "            #tower tucohy\n",
    "            bool_tuc = any(sd['label']=='tower_tucohy' for sd in json_file['shapes']) \n",
    "            if bool_tuc:\n",
    "                df.at[apples_indices,'tower_tucohy']= 1\n",
    "            else:\n",
    "                df.at[apples_indices,'tower_tucohy']= 0\n",
    "######################################################################################################\n",
    "#####################################################################################################                  df.at[apples_indices,'tower_tucohy']= 0\n",
    "\n",
    "#create column containing info for all of the towers\n",
    "df['towers'] = df[['tower_wooden', 'tower_lattice','tower_tucohy']].values.tolist()\n",
    "df['tower_labels'] = np.empty((len(df), 0)).tolist()\n",
    "for i in range(len(df)):\n",
    "\n",
    "    if df['towers'][i][0] == 1:\n",
    "        df['tower_labels'][i].append(\"tower_wooden\")\n",
    "\n",
    "    if df['towers'][i][1] == 1:\n",
    "        df['tower_labels'][i].append(\"tower_lattice\")\n",
    "\n",
    "    if df['towers'][i][2] == 1:\n",
    "        df['tower_labels'][i].append(\"tower_tucohy\")\n",
    "  ######################################################################################################\n",
    "#####################################################################################################        \n",
    "#need to split the data into testing, training, and validating\n",
    "random.seed(123)\n",
    "train, validate, test = \\\n",
    "              np.split(df.sample(frac=1, random_state=42), \n",
    "                       [int(.6*len(df)), int(.8*len(df))])    \n",
    "#reading in all of the training images\n",
    "train_image = []\n",
    "for i in tqdm(range(train.shape[0])):\n",
    "    img = image.load_img('C:/Users/miapi/Downloads/ttpla_dataset/sized_data - Copy/'+train.iloc[i][0],target_size=(400,400,3))\n",
    "    img = image.img_to_array(img)\n",
    "    img = img/255\n",
    "    train_image.append(img)\n",
    "X = np.array(train_image)\n",
    "#reading in all of the validation images\n",
    "valid_image = []\n",
    "for i in tqdm(range(validate.shape[0])):\n",
    "    img = image.load_img('C:/Users/miapi/Downloads/ttpla_dataset/sized_data - Copy/'+validate.iloc[i][0],target_size=(400,400,3))\n",
    "    img = image.img_to_array(img)\n",
    "    img = img/255\n",
    "    valid_image.append(img)\n",
    "X_valid = np.array(valid_image)\n",
    "#reading in all of the test images\n",
    "test_image = []\n",
    "for i in tqdm(range(test.shape[0])):\n",
    "    img = image.load_img('C:/Users/miapi/Downloads/ttpla_dataset/sized_data - Copy/'+test.iloc[i][0],target_size=(400,400,3))\n",
    "    img = image.img_to_array(img)\n",
    "    img = img/255\n",
    "    test_image.append(img)\n",
    "X_test = np.array(test_image)\n",
    "#need to df['DataFrame Column'] = df['DataFrame Column'].astype(float)first convert the column values from strings to floats\n",
    "df['tower_wooden'] = df['tower_wooden'].astype(int)\n",
    "df['tower_lattice'] = df['tower_lattice'].astype(int)\n",
    "df['tower_tucohy'] = df['tower_tucohy'].astype(int)\n",
    "y = np.array(train.drop(['Id', 'labels','towers','tower_labels'],axis=1))\n",
    "\n",
    "#validation set y\n",
    "y_validate = np.array(validate.drop(['Id', 'labels','towers','tower_labels'],axis=1))\n",
    "#test set y\n",
    "y_test = np.array(test.drop(['Id', 'labels','towers','tower_labels'],axis=1))\n",
    "\n",
    "\n",
    "X_train=np.asarray(X).astype(np.int)\n",
    "y_train=np.asarray(y).astype(np.int)\n",
    "X_valid=np.asarray(X_valid).astype(np.int)\n",
    "y_valid=np.asarray(y_validate).astype(np.int)\n",
    "X_test=np.asarray(X_test).astype(np.int)\n",
    "y_test=np.asarray(y_test).astype(np.int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['liststring'] = df['tower_labels'].apply(lambda x: ' '.join(map(str, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['liststring'] == '', 'liststring'] = 'none' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mapping for the image labels - \n",
    "#### following tutorial: Multi-Label Classification of Satellite Photos of the Amazon Rainforest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just want labels and liststring\n",
    "mapping_df = df[['labels','liststring']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#renaming columns to match tutorial\n",
    "mapping_df = mapping_df.rename(columns={\"labels\": \"image_name\", \"liststring\": \"tags\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting data into 3 sets\n",
    "random.seed(123)\n",
    "train, validate, test = \\\n",
    "              np.split(mapping_df.sample(frac=1, random_state=42), \n",
    "                       [int(.6*len(mapping_df)), int(.8*len(mapping_df))])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to reindex\n",
    "train = train.reset_index(drop=True)\n",
    "validate = validate.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the image labels \n",
    "labels = set()\n",
    "for i in range(len(mapping_df)):\n",
    "    tags = mapping_df['tags'][i].split(' ')\n",
    "    labels.update(tags)\n",
    "\n",
    "labels = list(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the mapping\n",
    "labels_map = {labels[i]:i for i in range(len(labels))}\n",
    "inv_labels_map = {i:labels[i] for i in range(len(labels))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#putting the mapping code into a function\n",
    "\n",
    "def create_tag_mapping(mapping_csv):\n",
    "        labels = set()\n",
    "        for i in range(len(mapping_csv)):\n",
    "            tags = mapping_csv['tags'][i].split(' ')\n",
    "            labels.update(tags)\n",
    "        labels = list(labels)\n",
    "        labels.sort()\n",
    "        labels_map = {labels[i]:i for i in range(len(labels))}\n",
    "        inv_labels_map = {i:labels[i] for i in range(len(labels))}\n",
    "        return labels_map, inv_labels_map\n",
    "\n",
    "\n",
    "def create_file_mapping(mapping_csv):\n",
    "    mapping = dict()\n",
    "    for i in range(len(mapping_csv)):\n",
    "        name, tags = mapping_csv['image_name'][i], mapping_csv['tags'][i]\n",
    "        mapping[name] = tags.split(' ')\n",
    "    return mapping\n",
    "\n",
    "# create a one hot encoding for one list of tags\n",
    "def one_hot_encode(tags, mapping):\n",
    "    encoding = zeros(len(mapping), dtype='uint8')\n",
    "    for tag in tags:\n",
    "        encoding[mapping[tag]] = 1\n",
    "    return encoding\n",
    "\n",
    "\n",
    "# load all images into memory\n",
    "def load_dataset(file_mapping, tag_mapping, data):\n",
    "    photos, targets = list(), list()\n",
    "    for i in tqdm(range(data.shape[0])):\n",
    "        filename = data.iloc[i][0]\n",
    "        photo = load_img('C:/Users/miapi/Downloads/ttpla_dataset/sized_data - Copy/'+filename+'.jpg',target_size=(400,400,3))\n",
    "        photo = img_to_array(photo, dtype='uint8')\n",
    "        tags = file_mapping[filename]\n",
    "        target = one_hot_encode(tags, tag_mapping)\n",
    "        photos.append(photo)\n",
    "        targets.append(target)\n",
    "    X = asarray(photos, dtype='uint8')\n",
    "    y = asarray(targets, dtype='uint8')\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading the train/test/validation sets in new mapping way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mapping_csv = train\n",
    "train_tag_mapping, _ = create_tag_mapping(train_mapping_csv)\n",
    "train_file_mapping = create_file_mapping(train_mapping_csv)\n",
    "\n",
    "trainX, trainY = load_dataset(train_file_mapping, train_tag_mapping, train)\n",
    "print(trainX.shape, trainY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_mapping_csv = validate\n",
    "valid_tag_mapping, _ = create_tag_mapping(valid_mapping_csv)\n",
    "valid_file_mapping = create_file_mapping(valid_mapping_csv)\n",
    "\n",
    "validX, validY = load_dataset(valid_file_mapping, valid_tag_mapping, validate)\n",
    "print(validX.shape, validY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mapping_csv = test\n",
    "test_tag_mapping, _ = create_tag_mapping(test_mapping_csv)\n",
    "test_file_mapping = create_file_mapping(test_mapping_csv)\n",
    "\n",
    "testX, testY = load_dataset(test_file_mapping, test_tag_mapping, test)\n",
    "print(testX.shape, testY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_yhat = asarray([ones(trainY.shape[1]) for _ in range(trainY.shape[0])])\n",
    "valid_yhat = asarray([ones(validY.shape[1]) for _ in range(validY.shape[0])])\n",
    "test_yhat = asarray([ones(testY.shape[1]) for _ in range(testY.shape[0])])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#performance metric\n",
    "def fbeta(y_true, y_pred, beta=2):\n",
    "    y_pred = backend.clip(y_pred, 0, 1)\n",
    "    tp = backend.sum(backend.round(backend.clip(y_true * y_pred, 0, 1)), axis=1)\n",
    "    fp = backend.sum(backend.round(backend.clip(y_pred - y_true, 0, 1)), axis=1)\n",
    "    fn = backend.sum(backend.round(backend.clip(y_true - y_pred, 0, 1)), axis=1)\n",
    "    p = tp / (tp + fp + backend.epsilon())\n",
    "    r = tp / (tp + fn + backend.epsilon())\n",
    "    bb = beta ** 2\n",
    "    fbeta_score = backend.mean((1 + bb) * (p * r) / (bb * p + r + backend.epsilon()))\n",
    "    return fbeta_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG- Type Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define vgg-type cnn model\n",
    "def define_model(in_shape=(400, 400, 3), out_shape=4):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=in_shape))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(out_shape, activation='sigmoid'))\n",
    "    # compile model\n",
    "    opt = SGD(lr=0.01, momentum=0.9)\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=[fbeta])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "train_it = datagen.flow(trainX, trainY, batch_size=32)\n",
    "valid_it = datagen.flow(validX, validY, batch_size=32)\n",
    "test_it = datagen.flow(testX, testY, batch_size=32)\n",
    "\n",
    "model = define_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(train_it, steps_per_epoch=len(train_it),\n",
    "    validation_data=valid_it, validation_steps=len(valid_it), epochs=50, verbose=1)\n",
    "\n",
    "loss, fbeta = model.evaluate_generator(test_it, steps=len(test_it), verbose=1)\n",
    "print('> loss=%.3f, fbeta=%.3f' % (loss, fbeta))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss\n",
    "pyplot.subplot(211)\n",
    "pyplot.title('Cross Entropy Loss')\n",
    "pyplot.plot(history.history['loss'], color='blue', label='train')\n",
    "pyplot.plot(history.history['val_loss'], color='orange', label='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot accuracy (F2)\n",
    "pyplot.subplot(212)\n",
    "pyplot.title('Fbeta')\n",
    "pyplot.plot(history.history['fbeta'], color='blue', label='train')\n",
    "pyplot.plot(history.history['val_fbeta'], color='orange', label='validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improve Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dropout regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(in_shape=(400, 400, 3), out_shape=4):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=in_shape))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(out_shape, activation='sigmoid'))\n",
    "    # compile model\n",
    "    opt = SGD(lr=0.01, momentum=0.9)\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=[fbeta])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "train_it = datagen.flow(trainX, trainY, batch_size=32)\n",
    "valid_it = datagen.flow(validX, validY, batch_size=32)\n",
    "test_it = datagen.flow(testX, testY, batch_size=32)\n",
    "\n",
    "model = define_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(train_it, steps_per_epoch=len(train_it),\n",
    "    validation_data=valid_it, validation_steps=len(valid_it), epochs=50, verbose=1)\n",
    "\n",
    "loss, fbeta = model.evaluate_generator(test_it, steps=len(test_it), verbose=1)\n",
    "print('> loss=%.3f, fbeta=%.3f' % (loss, fbeta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss\n",
    "pyplot.subplot(211)\n",
    "pyplot.title('Cross Entropy Loss')\n",
    "pyplot.plot(history.history['loss'], color='blue', label='train')\n",
    "pyplot.plot(history.history['val_loss'], color='orange', label='validation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot accuracy (F2)\n",
    "pyplot.subplot(212)\n",
    "pyplot.title('Fbeta')\n",
    "pyplot.plot(history.history['fbeta'], color='blue', label='train')\n",
    "pyplot.plot(history.history['val_fbeta'], color='orange', label='validation'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the VGG-type define model function\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255.0, horizontal_flip=True, vertical_flip=True, rotation_range=90)\n",
    "valid_datagen = ImageDataGenerator(rescale=1.0/255.0, horizontal_flip=True, vertical_flip=True, rotation_range=90)\n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "train_it = train_datagen.flow(trainX, trainY, batch_size=32)\n",
    "valid_it = valid_datagen.flow(validX, validY, batch_size=32)\n",
    "test_it = test_datagen.flow(testX, testY, batch_size=32)\n",
    "\n",
    "model=define_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(train_it, steps_per_epoch=len(train_it),\n",
    "    validation_data=valid_it, validation_steps=len(valid_it), epochs=50, verbose=1)\n",
    "\n",
    "loss, fbeta = model.evaluate_generator(test_it, steps=len(test_it), verbose=1)\n",
    "print('> loss=%.3f, fbeta=%.3f' % (loss, fbeta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss\n",
    "pyplot.subplot(211)\n",
    "pyplot.title('Cross Entropy Loss')\n",
    "pyplot.plot(history.history['loss'], color='blue', label='train')\n",
    "pyplot.plot(history.history['val_loss'], color='orange', label='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot accuracy (F2)\n",
    "pyplot.subplot(212)\n",
    "pyplot.title('Fbeta')\n",
    "pyplot.plot(history.history['fbeta'], color='blue', label='train')\n",
    "pyplot.plot(history.history['val_fbeta'], color='orange', label='validation'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine dropout and augmentation with more epochs (100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use dropout model\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255.0, horizontal_flip=True, vertical_flip=True, rotation_range=90)\n",
    "valid_datagen = ImageDataGenerator(rescale=1.0/255.0, horizontal_flip=True, vertical_flip=True, rotation_range=90)\n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "train_it = train_datagen.flow(trainX, trainY, batch_size=32)\n",
    "valid_it = valid_datagen.flow(validX, validY, batch_size=32)\n",
    "test_it = test_datagen.flow(testX, testY, batch_size=32)\n",
    "\n",
    "model = define_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(train_it, steps_per_epoch=len(train_it),\n",
    "    validation_data=valid_it, validation_steps=len(valid_it), epochs=100, verbose=1)\n",
    "\n",
    "loss, fbeta = model.evaluate_generator(test_it, steps=len(test_it), verbose=1)\n",
    "print('> loss=%.3f, fbeta=%.3f' % (loss, fbeta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss\n",
    "pyplot.subplot(211)\n",
    "pyplot.title('Cross Entropy Loss')\n",
    "pyplot.plot(history.history['loss'], color='blue', label='train')\n",
    "pyplot.plot(history.history['val_loss'], color='orange', label='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot accuracy (F2)\n",
    "pyplot.subplot(212)\n",
    "pyplot.title('Fbeta')\n",
    "pyplot.plot(history.history['fbeta'], color='blue', label='train')\n",
    "pyplot.plot(history.history['val_fbeta'], color='orange', label='validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(in_shape=(400, 400, 3), out_shape=4):\n",
    "    model = VGG16(include_top=False, input_shape=in_shape)\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    flat1 = Flatten()(model.layers[-1].output)\n",
    "    class1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "    output = Dense(out_shape, activation='sigmoid')(class1)\n",
    "\n",
    "    model = Model(inputs=model.inputs, outputs=output)\n",
    "\n",
    "    opt = SGD(lr=0.01, momentum=0.9)\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=[fbeta])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "datagen = ImageDataGenerator(featurewise_center=True)\n",
    "datagen.mean = [123.68, 116.779, 103.939]\n",
    "\n",
    "train_it = datagen.flow(trainX, trainY, batch_size=16)\n",
    "test_it = datagen.flow(testX, testY, batch_size=16)\n",
    "\n",
    "model = define_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(train_it, steps_per_epoch=len(train_it),\n",
    "    validation_data=valid_it, validation_steps=len(valid_it), epochs=100, verbose=1)\n",
    "\n",
    "loss, fbeta = model.evaluate_generator(test_it, steps=len(test_it), verbose=1)\n",
    "print('> loss=%.3f, fbeta=%.3f' % (loss, fbeta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss\n",
    "pyplot.subplot(211)\n",
    "pyplot.title('Cross Entropy Loss')\n",
    "pyplot.plot(history.history['loss'], color='blue', label='train')\n",
    "pyplot.plot(history.history['val_loss'], color='orange', label='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot accuracy (F2)\n",
    "pyplot.subplot(212)\n",
    "pyplot.title('Fbeta')\n",
    "pyplot.plot(history.history['fbeta'], color='blue', label='train')\n",
    "pyplot.plot(history.history['val_fbeta'], color='orange', label='validation')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
