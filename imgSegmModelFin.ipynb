{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "imageSegmentationModel.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOm3luczdHCTxElTLwFWKFx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/miapienkowski/cs254FinalProject/blob/Mia/imgSegmModelFin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Py9-j0exa1oT"
      },
      "source": [
        "from pycocotools.coco import COCO\n",
        "import numpy as np\n",
        "import skimage.io as io\n",
        "import random\n",
        "import os\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "%matplotlib inline\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHrlLWgpa4hi",
        "outputId": "40465df3-93a6-46a1-b4d7-c2b948373688"
      },
      "source": [
        "#import data from git repository\n",
        "!git clone https://github.com/miapienkowski/cs254FinalProject.git"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'cs254FinalProject' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmbRYv_xa-JP"
      },
      "source": [
        "#filterDataset processes data\n",
        "\n",
        "#mode train or validating or testing\n",
        "def filterDataset(folder, classes=None, mode='train'):    \n",
        "    # initialize COCO api for instance annotations\n",
        "    annFile = '{}/{}val.json'.format(folder, mode)\n",
        "    coco = COCO(annFile)\n",
        "    \n",
        "    images = []\n",
        "    if classes!=None:\n",
        "        # iterate for each individual class in the list\n",
        "        for className in classes:\n",
        "            # get all images containing given categories\n",
        "            catIds = coco.getCatIds(catNms=className)\n",
        "            imgIds = coco.getImgIds(catIds=catIds)\n",
        "            images += coco.loadImgs(imgIds)\n",
        "    \n",
        "    else:\n",
        "        imgIds = coco.getImgIds()\n",
        "        images = coco.loadImgs(imgIds)\n",
        "    \n",
        "    # Now, filter out the repeated images\n",
        "    unique_images = []\n",
        "    for i in range(len(images)):\n",
        "        if images[i] not in unique_images:\n",
        "            unique_images.append(images[i])\n",
        "            \n",
        "    random.shuffle(unique_images)\n",
        "    dataset_size = len(unique_images)\n",
        "    \n",
        "    return unique_images, dataset_size, coco"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2v_ZV6rebFll"
      },
      "source": [
        "#image processeing\n",
        "def getImage(imageObj, img_folder, input_image_size):\n",
        "    # Read and normalize an image\n",
        "    train_img = io.imread(img_folder + '/' + imageObj['file_name'])/255.0\n",
        "    # Resize\n",
        "    train_img = cv2.resize(train_img, input_image_size)\n",
        "    if (len(train_img.shape)==3 and train_img.shape[2]==3): # If it is a RGB 3 channel image\n",
        "        return train_img\n",
        "    else: # To handle a black and white image, increase dimensions to 3\n",
        "        stacked_img = np.stack((train_img,)*3, axis=-1)\n",
        "        return stacked_img\n",
        "\n",
        "#normal mask : 2-channel semantic segmentation mask with dimensions equal to the original image    \n",
        "def getNormalMask(imageObj, classes, coco, catIds, input_image_size):\n",
        "    annIds = coco.getAnnIds(imageObj['id'], catIds=catIds, iscrowd=None)\n",
        "    anns = coco.loadAnns(annIds)\n",
        "    cats = coco.loadCats(catIds)\n",
        "    train_mask = np.zeros(input_image_size)\n",
        "    for a in range(len(anns)):\n",
        "        className = getClassName(anns[a]['category_id'], cats)\n",
        "        pixel_value = classes.index(className)+1\n",
        "        new_mask = cv2.resize(coco.annToMask(anns[a])*pixel_value, input_image_size)\n",
        "        train_mask = np.maximum(new_mask, train_mask)\n",
        "\n",
        "    # Add extra dimension for parity with train_img size [X * X * 3]\n",
        "    train_mask = train_mask.reshape(input_image_size[0], input_image_size[1], 1)\n",
        "    return train_mask  \n",
        "#binary mask: 2 pixel values- 1 (object: could be any of the N classes) and 0 (the background)   \n",
        "def getBinaryMask(imageObj, coco, catIds, input_image_size):\n",
        "    annIds = coco.getAnnIds(imageObj['id'], catIds=catIds, iscrowd=None)\n",
        "    anns = coco.loadAnns(annIds)\n",
        "    train_mask = np.zeros(input_image_size)\n",
        "    for a in range(len(anns)):\n",
        "        new_mask = cv2.resize(coco.annToMask(anns[a]), input_image_size)\n",
        "        \n",
        "        #Threshold because resizing may cause extraneous values\n",
        "        new_mask[new_mask >= 0.5] = 1\n",
        "        new_mask[new_mask < 0.5] = 0\n",
        "\n",
        "        train_mask = np.maximum(new_mask, train_mask)\n",
        "\n",
        "    # Add extra dimension for parity with train_img size [X * X * 3]\n",
        "    train_mask = train_mask.reshape(input_image_size[0], input_image_size[1], 1)\n",
        "    return train_mask"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7VVUqVKbKcj"
      },
      "source": [
        "#builds generator type object\n",
        "#performs batch-sized loops [lines 20~] and retrieves the image from the folder,\n",
        "# loads the mask with the filtered outputs (binary or normal segmentation masks), \n",
        "# lumps them to form an image batch and mask batch\n",
        "\n",
        "def dataGeneratorCoco(images, classes, coco, folder, \n",
        "                      input_image_size=(224,224), batch_size=4, mode='train', mask_type='binary'):\n",
        "    \n",
        "\n",
        "    img_folder = '{}/data'.format(folder)\n",
        "    dataset_size = len(images)\n",
        "    catIds = coco.getCatIds(catNms=classes)\n",
        "    \n",
        "    c = 0\n",
        "    while(True):\n",
        "        img = np.zeros((batch_size, input_image_size[0], input_image_size[1], 3)).astype('float')\n",
        "        mask = np.zeros((batch_size, input_image_size[0], input_image_size[1], 1)).astype('float')\n",
        "\n",
        "        for i in range(c, c+batch_size): #initially from 0 to batch_size, when c = 0\n",
        "            imageObj = images[i]\n",
        "            \n",
        "            ### Retrieve Image ###\n",
        "            train_img = getImage(imageObj, img_folder, input_image_size)\n",
        "            \n",
        "            ### Create Mask ###\n",
        "            if mask_type==\"binary\":\n",
        "                train_mask = getBinaryMask(imageObj, coco, catIds, input_image_size)\n",
        "            \n",
        "            elif mask_type==\"normal\":\n",
        "                train_mask = getNormalMask(imageObj, classes, coco, catIds, input_image_size)                \n",
        "            \n",
        "            # Add to respective batch sized arrays\n",
        "            img[i-c] = train_img\n",
        "            mask[i-c] = train_mask\n",
        "            \n",
        "        c+=batch_size\n",
        "        if(c + batch_size >= dataset_size):\n",
        "            c=0\n",
        "            random.shuffle(images)\n",
        "        yield img, mask"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yfWh6fUmtOy"
      },
      "source": [
        "def getImageMask(images, classes, coco, folder, \n",
        "                      input_image_size=(224,224), batch_size=4, mode='train', mask_type='binary'):\n",
        "    \n",
        "\n",
        "    img_folder = '{}/data'.format(folder)\n",
        "    dataset_size = len(images)\n",
        "    catIds = coco.getCatIds(catNms=classes)\n",
        "    \n",
        "    c = 154\n",
        "    while(True):\n",
        "        img = np.zeros((batch_size, input_image_size[0], input_image_size[1], 3)).astype('float')\n",
        "        mask = np.zeros((batch_size, input_image_size[0], input_image_size[1], 1)).astype('float')\n",
        "\n",
        "        for i in range(c, c+batch_size): #initially from 0 to batch_size, when c = 0\n",
        "            imageObj = images[i]\n",
        "            \n",
        "            ### Retrieve Image ###\n",
        "            train_img = getImage(imageObj, img_folder, input_image_size)\n",
        "            \n",
        "            ### Create Mask ###\n",
        "            if mask_type==\"binary\":\n",
        "                train_mask = getBinaryMask(imageObj, coco, catIds, input_image_size)\n",
        "            \n",
        "            elif mask_type==\"normal\":\n",
        "                train_mask = getNormalMask(imageObj, classes, coco, catIds, input_image_size)                \n",
        "            \n",
        "            # Add to respective batch sized arrays\n",
        "            img[i-c] = train_img\n",
        "            mask[i-c] = train_mask\n",
        "            \n",
        "        c+=batch_size\n",
        "        if(c + batch_size >= dataset_size):\n",
        "            c=0\n",
        "            random.shuffle(images)\n",
        "        return img, mask"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJmbS_V0lHYX"
      },
      "source": [
        "def show_predictions(dataset=None, num=1):\n",
        "  if dataset:\n",
        "    for image, mask in dataset.take(num):\n",
        "      pred_mask = model.predict(image)\n",
        "      display([image[0], mask[0], create_mask(pred_mask)])\n",
        "  else:\n",
        "    display([sample_image, sample_mask,\n",
        "             create_mask(model.predict(sample_image[tf.newaxis, ...]))])"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZQT0NT7bNom",
        "outputId": "3e9b9b0d-1454-40fb-c08b-47510a952d59"
      },
      "source": [
        "#get processed validating data\n",
        "#validating\n",
        "val_folder = 'cs254FinalProject/validatingDirectory'\n",
        "val_classes = ['cable', 'tower_lattice', 'tower_tuchoy','tower_wooden']\n",
        "val_mode = 'validating'\n",
        "\n",
        "val_images, val_dataset_size, val_coco = filterDataset(val_folder, val_classes, val_mode)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.02s)\n",
            "creating index...\n",
            "index created!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8jTgFk8bSy_",
        "outputId": "cb2c3312-5400-4a16-d1ac-c2d22f7311b4"
      },
      "source": [
        "#get processed training data\n",
        "#training\n",
        "train_folder = 'cs254FinalProject/trainingDirectory'\n",
        "train_classes = ['cable', 'tower_lattice', 'tower_tuchoy','tower_wooden']\n",
        "train_mode = 'train'\n",
        "\n",
        "train_images, train_dataset_size, train_coco = filterDataset(train_folder, train_classes, train_mode)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.19s)\n",
            "creating index...\n",
            "index created!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRmHc8S5lO6Q",
        "outputId": "01966dad-c308-471a-bd6a-52dea754e806"
      },
      "source": [
        "#get processed training data\n",
        "#training\n",
        "test_folder = 'cs254FinalProject/testingDirectory'\n",
        "test_classes = ['cable', 'tower_lattice', 'tower_tuchoy','tower_wooden']\n",
        "test_mode = 'test'\n",
        "\n",
        "test_images, test_dataset_size, test_coco = filterDataset(test_folder, test_classes, test_mode)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.24s)\n",
            "creating index...\n",
            "index created!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yInf0JFbXf_"
      },
      "source": [
        "#creates a data generator object yielding batches of images and binary mask\n",
        "#val\n",
        "batch_size = 4\n",
        "input_image_size = (224,224)\n",
        "mask_type = 'binary'\n",
        "\n",
        "val_gen = dataGeneratorCoco(val_images, val_classes, val_coco, val_folder, \n",
        "                            input_image_size, batch_size, val_mode, mask_type)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihH3RIzTbcOb"
      },
      "source": [
        "#creates a data generator object yielding batches of images and binary mask\n",
        "#train\n",
        "batch_size = 4\n",
        "input_image_size = (224,224)\n",
        "mask_type = 'binary'\n",
        "\n",
        "train_gen = dataGeneratorCoco(train_images, train_classes, train_coco, train_folder, \n",
        "                            input_image_size, batch_size, train_mode, mask_type)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3QVumYTllOX"
      },
      "source": [
        "#creates a data generator object yielding batches of images and binary mask\n",
        "#test\n",
        "batch_size = 4\n",
        "input_image_size = (224,224)\n",
        "mask_type = 'binary'\n",
        "\n",
        "test_gen = dataGeneratorCoco(test_images, test_classes, test_coco, test_folder, \n",
        "                            input_image_size, batch_size, test_mode, mask_type)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6H3-v-QKuBy"
      },
      "source": [
        "batch_size = 4\n",
        "input_image_size = (224,224)\n",
        "mask_type = 'binary'\n",
        "\n",
        "tIm, Tms = getImageMask(test_images, test_classes, test_coco, test_folder, \n",
        "                            input_image_size, batch_size, test_mode, mask_type)"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwKeiPNQbfqy"
      },
      "source": [
        "n_epochs = 10\n",
        "steps_per_epoch = train_dataset_size// batch_size\n",
        "validation_steps = val_dataset_size // batch_size"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rfa97rhrblZc",
        "outputId": "9a12e59e-9c4e-4376-e21d-3f25892b9f07"
      },
      "source": [
        "!pip install git+https://github.com/tensorflow/examples.git"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/tensorflow/examples.git\n",
            "  Cloning https://github.com/tensorflow/examples.git to /tmp/pip-req-build-hs0xpscm\n",
            "  Running command git clone -q https://github.com/tensorflow/examples.git /tmp/pip-req-build-hs0xpscm\n",
            "Requirement already satisfied (use --upgrade to upgrade): tensorflow-examples===a4febbbd69d2632e7ed28e5a58f8473d814431b3- from git+https://github.com/tensorflow/examples.git in /usr/local/lib/python3.7/dist-packages\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from tensorflow-examples===a4febbbd69d2632e7ed28e5a58f8473d814431b3-) (0.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorflow-examples===a4febbbd69d2632e7ed28e5a58f8473d814431b3-) (1.15.0)\n",
            "Building wheels for collected packages: tensorflow-examples\n",
            "  Building wheel for tensorflow-examples (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tensorflow-examples: filename=tensorflow_examples-a4febbbd69d2632e7ed28e5a58f8473d814431b3_-cp37-none-any.whl size=225469 sha256=66eaa6064b35e3e2c878b744e9f042b9e0364e557bd04a19ec0590f66a6b7e3e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-1vyyo14c/wheels/83/64/b3/4cfa02dc6f9d16bf7257892c6a7ec602cd7e0ff6ec4d7d714d\n",
            "Successfully built tensorflow-examples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvEtB7HFbqzR"
      },
      "source": [
        "from tensorflow_examples.models.pix2pix import pix2pix\n",
        "import tensorflow as tf"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GjehVIcbu4D"
      },
      "source": [
        "OUTPUT_CHANNELS = 3\n",
        "base_model = tf.keras.applications.MobileNetV2(input_shape=[224, 224, 3], include_top=False)\n",
        "\n",
        "# Use the activations of these layers\n",
        "layer_names = [\n",
        "    'block_1_expand_relu',   # 64x64\n",
        "    'block_3_expand_relu',   # 32x32\n",
        "    'block_6_expand_relu',   # 16x16\n",
        "    'block_13_expand_relu',  # 8x8\n",
        "    'block_16_project',      # 4x4\n",
        "]\n",
        "base_model_outputs = [base_model.get_layer(name).output for name in layer_names]\n",
        "\n",
        "# Create the feature extraction model\n",
        "down_stack = tf.keras.Model(inputs=base_model.input, outputs=base_model_outputs)\n",
        "\n",
        "down_stack.trainable = False"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_HOcIfrbybU"
      },
      "source": [
        "up_stack = [\n",
        "    pix2pix.upsample(512, 3),  # 4x4 -> 8x8\n",
        "    pix2pix.upsample(256, 3),  # 8x8 -> 16x16\n",
        "    pix2pix.upsample(128, 3),  # 16x16 -> 32x32\n",
        "    pix2pix.upsample(64, 3),   # 32x32 -> 64x64\n",
        "]"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSn8IZiib1Pr"
      },
      "source": [
        "def unet_model(output_channels):\n",
        "  inputs = tf.keras.layers.Input(shape=[224, 224, 3])\n",
        "\n",
        "  # Downsampling through the model\n",
        "  skips = down_stack(inputs)\n",
        "  x = skips[-1]\n",
        "  skips = reversed(skips[:-1])\n",
        "\n",
        "  # Upsampling and establishing the skip connections\n",
        "  for up, skip in zip(up_stack, skips):\n",
        "    x = up(x)\n",
        "    concat = tf.keras.layers.Concatenate()\n",
        "    x = concat([x, skip])\n",
        "\n",
        "  # This is the last layer of the model\n",
        "  last = tf.keras.layers.Conv2DTranspose(\n",
        "      output_channels, 3, strides=2,\n",
        "      padding='same')  #64x64 -> 128x128\n",
        "\n",
        "  x = last(x)\n",
        "\n",
        "  return tf.keras.Model(inputs=inputs, outputs=x)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiv-givhb4Wj"
      },
      "source": [
        "model = unet_model(OUTPUT_CHANNELS)\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPwCYKX5b6qk",
        "outputId": "3a471f9f-6ede-4cba-dc55-6d30c629bafc"
      },
      "source": [
        "model_history = model.fit(x = train_gen,\n",
        "                validation_data = val_gen,\n",
        "                steps_per_epoch = steps_per_epoch,\n",
        "                validation_steps = validation_steps,\n",
        "                epochs = n_epochs,\n",
        "                verbose=True)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "226/226 [==============================] - 199s 865ms/step - loss: 0.1866 - accuracy: 0.9783 - val_loss: 0.1236 - val_accuracy: 0.9493\n",
            "Epoch 2/10\n",
            "226/226 [==============================] - 196s 867ms/step - loss: 0.1060 - accuracy: 0.9743 - val_loss: 0.0997 - val_accuracy: 0.9656\n",
            "Epoch 3/10\n",
            "226/226 [==============================] - 195s 864ms/step - loss: 0.0971 - accuracy: 0.9660 - val_loss: 0.0973 - val_accuracy: 0.9656\n",
            "Epoch 4/10\n",
            "226/226 [==============================] - 193s 853ms/step - loss: 0.0848 - accuracy: 0.9672 - val_loss: 0.1023 - val_accuracy: 0.9820\n",
            "Epoch 5/10\n",
            "226/226 [==============================] - 195s 864ms/step - loss: 0.0849 - accuracy: 0.9628 - val_loss: 0.1419 - val_accuracy: 0.9803\n",
            "Epoch 6/10\n",
            "226/226 [==============================] - 196s 869ms/step - loss: 0.0825 - accuracy: 0.9682 - val_loss: 0.0936 - val_accuracy: 0.9750\n",
            "Epoch 7/10\n",
            "226/226 [==============================] - 195s 865ms/step - loss: 0.0791 - accuracy: 0.9634 - val_loss: 0.0950 - val_accuracy: 0.9773\n",
            "Epoch 8/10\n",
            "226/226 [==============================] - 195s 863ms/step - loss: 0.0737 - accuracy: 0.9649 - val_loss: 0.0957 - val_accuracy: 0.9686\n",
            "Epoch 9/10\n",
            "226/226 [==============================] - 195s 862ms/step - loss: 0.0790 - accuracy: 0.9583 - val_loss: 0.0829 - val_accuracy: 0.9645\n",
            "Epoch 10/10\n",
            "226/226 [==============================] - 194s 857ms/step - loss: 0.0670 - accuracy: 0.9630 - val_loss: 0.0972 - val_accuracy: 0.9582\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "YqEDZxREkWic",
        "outputId": "d5615f01-c286-4163-be34-47aab1f0d637"
      },
      "source": [
        "loss = model_history.history['loss']\n",
        "val_loss = model_history.history['val_loss']\n",
        "\n",
        "epochs = range(n_epochs)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'r', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'bo', label='Validation loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss Value')\n",
        "plt.ylim([0, 1])\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQNSpg_jnSra"
      },
      "source": [
        "\n",
        "predictions = model.predict(test_gen,3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gvpfB0Pk2Ri",
        "outputId": "16c7ebe6-89b3-4e8a-9904-7ec56fa8bd23"
      },
      "source": [
        "predictions = model.predict_generator(test_gen, 4)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1905: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
            "  warnings.warn('`Model.predict_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqOf54HTUg0e",
        "outputId": "08b4bee9-1b2e-4516-a216-38a40ab569e6"
      },
      "source": [
        "model.evaluate_generator(test_gen)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1877: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
            "  warnings.warn('`Model.evaluate_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}