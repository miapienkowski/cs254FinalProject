{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "imageSegmentationModel.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNsBjCHYPFhWqV7Y53YiRKC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/miapienkowski/cs254FinalProject/blob/Mia/imageSegmentationModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Py9-j0exa1oT"
      },
      "source": [
        "from pycocotools.coco import COCO\n",
        "import numpy as np\n",
        "import skimage.io as io\n",
        "import random\n",
        "import os\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "%matplotlib inline\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHrlLWgpa4hi",
        "outputId": "36021fa4-4437-47de-968f-0333f290e0c2"
      },
      "source": [
        "#import data from git repository\n",
        "!git clone https://github.com/miapienkowski/cs254FinalProject.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'cs254FinalProject'...\n",
            "remote: Enumerating objects: 9, done.\u001b[K\n",
            "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 17960 (delta 0), reused 4 (delta 0), pack-reused 17951\u001b[K\n",
            "Receiving objects: 100% (17960/17960), 548.22 MiB | 30.44 MiB/s, done.\n",
            "Resolving deltas: 100% (2167/2167), done.\n",
            "Checking out files: 100% (3731/3731), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmbRYv_xa-JP"
      },
      "source": [
        "#filterDataset processes data\n",
        "\n",
        "#mode train or validating or testing\n",
        "def filterDataset(folder, classes=None, mode='train'):    \n",
        "    # initialize COCO api for instance annotations\n",
        "    annFile = '{}/{}val.json'.format(folder, mode)\n",
        "    coco = COCO(annFile)\n",
        "    \n",
        "    images = []\n",
        "    if classes!=None:\n",
        "        # iterate for each individual class in the list\n",
        "        for className in classes:\n",
        "            # get all images containing given categories\n",
        "            catIds = coco.getCatIds(catNms=className)\n",
        "            imgIds = coco.getImgIds(catIds=catIds)\n",
        "            images += coco.loadImgs(imgIds)\n",
        "    \n",
        "    else:\n",
        "        imgIds = coco.getImgIds()\n",
        "        images = coco.loadImgs(imgIds)\n",
        "    \n",
        "    # Now, filter out the repeated images\n",
        "    unique_images = []\n",
        "    for i in range(len(images)):\n",
        "        if images[i] not in unique_images:\n",
        "            unique_images.append(images[i])\n",
        "            \n",
        "    random.shuffle(unique_images)\n",
        "    dataset_size = len(unique_images)\n",
        "    \n",
        "    return unique_images, dataset_size, coco"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2v_ZV6rebFll"
      },
      "source": [
        "#image processeing\n",
        "def getImage(imageObj, img_folder, input_image_size):\n",
        "    # Read and normalize an image\n",
        "    train_img = io.imread(img_folder + '/' + imageObj['file_name'])/255.0\n",
        "    # Resize\n",
        "    train_img = cv2.resize(train_img, input_image_size)\n",
        "    if (len(train_img.shape)==3 and train_img.shape[2]==3): # If it is a RGB 3 channel image\n",
        "        return train_img\n",
        "    else: # To handle a black and white image, increase dimensions to 3\n",
        "        stacked_img = np.stack((train_img,)*3, axis=-1)\n",
        "        return stacked_img\n",
        "\n",
        "#normal mask : 2-channel semantic segmentation mask with dimensions equal to the original image    \n",
        "def getNormalMask(imageObj, classes, coco, catIds, input_image_size):\n",
        "    annIds = coco.getAnnIds(imageObj['id'], catIds=catIds, iscrowd=None)\n",
        "    anns = coco.loadAnns(annIds)\n",
        "    cats = coco.loadCats(catIds)\n",
        "    train_mask = np.zeros(input_image_size)\n",
        "    for a in range(len(anns)):\n",
        "        className = getClassName(anns[a]['category_id'], cats)\n",
        "        pixel_value = classes.index(className)+1\n",
        "        new_mask = cv2.resize(coco.annToMask(anns[a])*pixel_value, input_image_size)\n",
        "        train_mask = np.maximum(new_mask, train_mask)\n",
        "\n",
        "    # Add extra dimension for parity with train_img size [X * X * 3]\n",
        "    train_mask = train_mask.reshape(input_image_size[0], input_image_size[1], 1)\n",
        "    return train_mask  \n",
        "#binary mask: 2 pixel values- 1 (object: could be any of the N classes) and 0 (the background)   \n",
        "def getBinaryMask(imageObj, coco, catIds, input_image_size):\n",
        "    annIds = coco.getAnnIds(imageObj['id'], catIds=catIds, iscrowd=None)\n",
        "    anns = coco.loadAnns(annIds)\n",
        "    train_mask = np.zeros(input_image_size)\n",
        "    for a in range(len(anns)):\n",
        "        new_mask = cv2.resize(coco.annToMask(anns[a]), input_image_size)\n",
        "        \n",
        "        #Threshold because resizing may cause extraneous values\n",
        "        new_mask[new_mask >= 0.5] = 1\n",
        "        new_mask[new_mask < 0.5] = 0\n",
        "\n",
        "        train_mask = np.maximum(new_mask, train_mask)\n",
        "\n",
        "    # Add extra dimension for parity with train_img size [X * X * 3]\n",
        "    train_mask = train_mask.reshape(input_image_size[0], input_image_size[1], 1)\n",
        "    return train_mask"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7VVUqVKbKcj"
      },
      "source": [
        "#builds generator type object\n",
        "#performs batch-sized loops [lines 20~] and retrieves the image from the folder,\n",
        "# loads the mask with the filtered outputs (binary or normal segmentation masks), \n",
        "# lumps them to form an image batch and mask batch\n",
        "\n",
        "def dataGeneratorCoco(images, classes, coco, folder, \n",
        "                      input_image_size=(224,224), batch_size=4, mode='train', mask_type='binary'):\n",
        "    \n",
        "\n",
        "    img_folder = '{}/data'.format(folder)\n",
        "    dataset_size = len(images)\n",
        "    catIds = coco.getCatIds(catNms=classes)\n",
        "    \n",
        "    c = 0\n",
        "    while(True):\n",
        "        img = np.zeros((batch_size, input_image_size[0], input_image_size[1], 3)).astype('float')\n",
        "        mask = np.zeros((batch_size, input_image_size[0], input_image_size[1], 1)).astype('float')\n",
        "\n",
        "        for i in range(c, c+batch_size): #initially from 0 to batch_size, when c = 0\n",
        "            imageObj = images[i]\n",
        "            \n",
        "            ### Retrieve Image ###\n",
        "            train_img = getImage(imageObj, img_folder, input_image_size)\n",
        "            \n",
        "            ### Create Mask ###\n",
        "            if mask_type==\"binary\":\n",
        "                train_mask = getBinaryMask(imageObj, coco, catIds, input_image_size)\n",
        "            \n",
        "            elif mask_type==\"normal\":\n",
        "                train_mask = getNormalMask(imageObj, classes, coco, catIds, input_image_size)                \n",
        "            \n",
        "            # Add to respective batch sized arrays\n",
        "            img[i-c] = train_img\n",
        "            mask[i-c] = train_mask\n",
        "            \n",
        "        c+=batch_size\n",
        "        if(c + batch_size >= dataset_size):\n",
        "            c=0\n",
        "            random.shuffle(images)\n",
        "        yield img, mask"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZQT0NT7bNom",
        "outputId": "1d7cfe96-27a5-4fcd-a695-50d4513c8019"
      },
      "source": [
        "#get processed validating data\n",
        "#validating\n",
        "val_folder = 'cs254FinalProject/validatingDirectory'\n",
        "val_classes = ['cable', 'tower_lattice', 'tower_tuchoy','tower_wooden']\n",
        "val_mode = 'validating'\n",
        "\n",
        "val_images, val_dataset_size, val_coco = filterDataset(val_folder, val_classes, val_mode)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.03s)\n",
            "creating index...\n",
            "index created!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8jTgFk8bSy_",
        "outputId": "f815564e-5165-43b1-9902-1fcd55524178"
      },
      "source": [
        "#get processed training data\n",
        "#training\n",
        "train_folder = 'cs254FinalProject/trainingDirectory'\n",
        "train_classes = ['cable', 'tower_lattice', 'tower_tuchoy','tower_wooden']\n",
        "train_mode = 'train'\n",
        "\n",
        "train_images, train_dataset_size, train_coco = filterDataset(train_folder, train_classes, train_mode)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.29s)\n",
            "creating index...\n",
            "index created!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yInf0JFbXf_"
      },
      "source": [
        "#creates a data generator object yielding batches of images and binary mask\n",
        "#val\n",
        "batch_size = 4\n",
        "input_image_size = (224,224)\n",
        "mask_type = 'binary'\n",
        "\n",
        "val_gen = dataGeneratorCoco(val_images, val_classes, val_coco, val_folder, \n",
        "                            input_image_size, batch_size, val_mode, mask_type)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihH3RIzTbcOb"
      },
      "source": [
        "#creates a data generator object yielding batches of images and binary mask\n",
        "#train\n",
        "batch_size = 4\n",
        "input_image_size = (224,224)\n",
        "mask_type = 'binary'\n",
        "\n",
        "train_gen = dataGeneratorCoco(train_images, train_classes, train_coco, train_folder, \n",
        "                            input_image_size, batch_size, train_mode, mask_type)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwKeiPNQbfqy"
      },
      "source": [
        "n_epochs = 10\n",
        "steps_per_epoch = train_dataset_size// batch_size\n",
        "validation_steps = val_dataset_size // batch_size"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rfa97rhrblZc",
        "outputId": "cf51a78e-afe1-44eb-ed47-2cc945d33eef"
      },
      "source": [
        "!pip install git+https://github.com/tensorflow/examples.git"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/tensorflow/examples.git\n",
            "  Cloning https://github.com/tensorflow/examples.git to /tmp/pip-req-build-6fy3s_9_\n",
            "  Running command git clone -q https://github.com/tensorflow/examples.git /tmp/pip-req-build-6fy3s_9_\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from tensorflow-examples===a4febbbd69d2632e7ed28e5a58f8473d814431b3-) (0.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorflow-examples===a4febbbd69d2632e7ed28e5a58f8473d814431b3-) (1.15.0)\n",
            "Building wheels for collected packages: tensorflow-examples\n",
            "  Building wheel for tensorflow-examples (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tensorflow-examples: filename=tensorflow_examples-a4febbbd69d2632e7ed28e5a58f8473d814431b3_-cp37-none-any.whl size=225469 sha256=85c110958773456a847cbb69abed61ba692b34924c8affa90b5a01779e4b2067\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-rqrmr4rx/wheels/83/64/b3/4cfa02dc6f9d16bf7257892c6a7ec602cd7e0ff6ec4d7d714d\n",
            "Successfully built tensorflow-examples\n",
            "Installing collected packages: tensorflow-examples\n",
            "Successfully installed tensorflow-examples-a4febbbd69d2632e7ed28e5a58f8473d814431b3-\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvEtB7HFbqzR"
      },
      "source": [
        "from tensorflow_examples.models.pix2pix import pix2pix\n",
        "import tensorflow as tf"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GjehVIcbu4D",
        "outputId": "5aa0a332-da30-4eb0-9db5-81b8ef2b2961"
      },
      "source": [
        "OUTPUT_CHANNELS = 3\n",
        "base_model = tf.keras.applications.MobileNetV2(input_shape=[224, 224, 3], include_top=False)\n",
        "\n",
        "# Use the activations of these layers\n",
        "layer_names = [\n",
        "    'block_1_expand_relu',   # 64x64\n",
        "    'block_3_expand_relu',   # 32x32\n",
        "    'block_6_expand_relu',   # 16x16\n",
        "    'block_13_expand_relu',  # 8x8\n",
        "    'block_16_project',      # 4x4\n",
        "]\n",
        "base_model_outputs = [base_model.get_layer(name).output for name in layer_names]\n",
        "\n",
        "# Create the feature extraction model\n",
        "down_stack = tf.keras.Model(inputs=base_model.input, outputs=base_model_outputs)\n",
        "\n",
        "down_stack.trainable = False"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "9412608/9406464 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_HOcIfrbybU"
      },
      "source": [
        "up_stack = [\n",
        "    pix2pix.upsample(512, 3),  # 4x4 -> 8x8\n",
        "    pix2pix.upsample(256, 3),  # 8x8 -> 16x16\n",
        "    pix2pix.upsample(128, 3),  # 16x16 -> 32x32\n",
        "    pix2pix.upsample(64, 3),   # 32x32 -> 64x64\n",
        "]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSn8IZiib1Pr"
      },
      "source": [
        "def unet_model(output_channels):\n",
        "  inputs = tf.keras.layers.Input(shape=[224, 224, 3])\n",
        "\n",
        "  # Downsampling through the model\n",
        "  skips = down_stack(inputs)\n",
        "  x = skips[-1]\n",
        "  skips = reversed(skips[:-1])\n",
        "\n",
        "  # Upsampling and establishing the skip connections\n",
        "  for up, skip in zip(up_stack, skips):\n",
        "    x = up(x)\n",
        "    concat = tf.keras.layers.Concatenate()\n",
        "    x = concat([x, skip])\n",
        "\n",
        "  # This is the last layer of the model\n",
        "  last = tf.keras.layers.Conv2DTranspose(\n",
        "      output_channels, 3, strides=2,\n",
        "      padding='same')  #64x64 -> 128x128\n",
        "\n",
        "  x = last(x)\n",
        "\n",
        "  return tf.keras.Model(inputs=inputs, outputs=x)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiv-givhb4Wj"
      },
      "source": [
        "model = unet_model(OUTPUT_CHANNELS)\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPwCYKX5b6qk",
        "outputId": "88e3b0c4-4b79-4d4e-ff9d-74583a3b5eed"
      },
      "source": [
        "model_history = model.fit(x = train_gen,\n",
        "                validation_data = val_gen,\n",
        "                steps_per_epoch = steps_per_epoch,\n",
        "                validation_steps = validation_steps,\n",
        "                epochs = n_epochs,\n",
        "                verbose=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "226/226 [==============================] - 196s 855ms/step - loss: 0.2944 - accuracy: 0.9228 - val_loss: 0.1240 - val_accuracy: 0.9417\n",
            "Epoch 2/10\n",
            "209/226 [==========================>...] - ETA: 13s - loss: 0.1074 - accuracy: 0.9731"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}